{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env with **Pytorch 1.6** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load KPs (See 4-2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_KPS_PTH = \"predicted_kps\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['predicted_kps/[0000]brush_hair.joblib',\n",
       "  'predicted_kps/[0001]brush_hair.joblib',\n",
       "  'predicted_kps/[0002]brush_hair.joblib',\n",
       "  'predicted_kps/[0003]brush_hair.joblib',\n",
       "  'predicted_kps/[0004]brush_hair.joblib'],\n",
       " ['predicted_kps/[6761]wave.joblib',\n",
       "  'predicted_kps/[6762]wave.joblib',\n",
       "  'predicted_kps/[6763]wave.joblib',\n",
       "  'predicted_kps/[6764]wave.joblib',\n",
       "  'predicted_kps/[6765]wave.joblib'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_filenames = sorted(glob.glob(f\"{LOAD_KPS_PTH}/*]*.joblib\"))\n",
    "kps_filenames[:5] , kps_filenames[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['width', 'height', 'frames_per_second', 'num_frames', 'num_role_register', 'boxes_buffer', 'kps_buffer', 'roles_buffer', 'frames_split_pos', 'cost_matrix_collection', 'image_size', 'bbox_area_by_role', 'num_frames_by_role', 'motion_distance_by_role', 'frames_by_role', 'completeness_by_role', 'selected_roles', 'boxes_buffer_selected', 'kps_buffer_selected'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_d = joblib.load(kps_filenames[1])\n",
    "info_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, (394, 17, 3), (394, 1, 17, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_d['num_frames'], info_d['kps_buffer'].shape, info_d['kps_buffer_selected'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_kps/[0000]brush_hair.joblib\n",
      "409 (415, 17, 3) (408, 1, 17, 3)\n",
      "  - num_role_register:  6\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0001]brush_hair.joblib\n",
      "395 (394, 17, 3) (394, 1, 17, 3)\n",
      "  - num_role_register:  1\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0002]brush_hair.joblib\n",
      "323 (322, 17, 3) (322, 1, 17, 3)\n",
      "  - num_role_register:  1\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0003]brush_hair.joblib\n",
      "246 (281, 17, 3) (245, 1, 17, 3)\n",
      "  - num_role_register:  26\n",
      "  - selected_roles:  [7]\n",
      "****************************************************************\n",
      "predicted_kps/[0004]brush_hair.joblib\n",
      "159 (169, 17, 3) (158, 1, 17, 3)\n",
      "  - num_role_register:  6\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "for kpfn in kps_filenames[:5]:\n",
    "    info_d = joblib.load(kpfn)\n",
    "    print(kpfn)\n",
    "    #print(info_d.keys())\n",
    "    print(info_d['num_frames'], info_d['kps_buffer'].shape, info_d['kps_buffer_selected'].shape)\n",
    "    print('  - num_role_register: ',info_d['num_role_register'])\n",
    "    #print('  - num_frames_by_role: ', info_d['num_frames_by_role'])\n",
    "    #print('  - motion_distance_by_role: ', info_d['motion_distance_by_role'])\n",
    "    #print('  - completeness_by_role: ', info_d['completeness_by_role'])\n",
    "    print('  - selected_roles: ', info_d['selected_roles'])\n",
    "    print('*'*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Selected Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5796/5796 [00:28<00:00, 200.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "video_selected = []\n",
    "num_roles_by_video = {}\n",
    "\n",
    "for j, kpfn in enumerate(tqdm.tqdm(kps_filenames)):\n",
    "        id_, class_ = kpfn.split('/')[-1].split('.')[0][1:].split(']') # \"6765\", \"wave\"\n",
    "        #print(id_, class_)\n",
    "        info_d = joblib.load(kpfn)\n",
    "        n_roles = len(info_d['num_frames_by_role'])\n",
    "        id_ = int(id_)\n",
    "        num_roles_by_video.update({id_:n_roles})\n",
    "        if n_roles==1:\n",
    "            video_selected.append(id_)\n",
    "        \n",
    "joblib.dump(video_selected, f\"demo/video_selected.joblib\")\n",
    "joblib.dump(num_roles_by_video, f\"demo/num_roles_by_video.joblib\")\n",
    "video_selected = joblib.load(f\"demo/video_selected.joblib\")\n",
    "num_roles_by_video = joblib.load(f\"demo/num_roles_by_video.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有[5796]支成功辨識的video\n",
      "選擇比例[27.59]%\n"
     ]
    }
   ],
   "source": [
    "print(f\"共有[{len(num_roles_by_video)}]支成功辨識的video\")\n",
    "print(\"選擇比例[{:.2f}]%\".format(len(video_selected)/ len(num_roles_by_video)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 10, 12, 17]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_selected[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_roles_by_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset Overview (See 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_0.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_1.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_2.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename       class  width  \\\n",
       "0          April_09_brush_hair_u_nm_np1_ba_goo_0.avi  brush_hair    320   \n",
       "1          April_09_brush_hair_u_nm_np1_ba_goo_1.avi  brush_hair    320   \n",
       "2          April_09_brush_hair_u_nm_np1_ba_goo_2.avi  brush_hair    320   \n",
       "3  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "4  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "\n",
       "   height  frames_per_second  num_frames  \n",
       "0     240               30.0         409  \n",
       "1     240               30.0         395  \n",
       "2     240               30.0         323  \n",
       "3     240               30.0         246  \n",
       "4     240               30.0         159  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df = pd.read_csv(\"demo/video_info.csv\", index_col=0)\n",
    "video_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>num_roles_in_video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_0.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>409</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_1.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>395</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_2.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>246</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>159</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename       class  width  \\\n",
       "0          April_09_brush_hair_u_nm_np1_ba_goo_0.avi  brush_hair    320   \n",
       "1          April_09_brush_hair_u_nm_np1_ba_goo_1.avi  brush_hair    320   \n",
       "2          April_09_brush_hair_u_nm_np1_ba_goo_2.avi  brush_hair    320   \n",
       "3  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "4  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "\n",
       "   height  frames_per_second  num_frames  num_roles_in_video  \n",
       "0     240               30.0         409                 6.0  \n",
       "1     240               30.0         395                 1.0  \n",
       "2     240               30.0         323                 1.0  \n",
       "3     240               30.0         246                26.0  \n",
       "4     240               30.0         159                 6.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "video_info_df['num_roles_in_video'] = \\\n",
    "    [ int(num_roles_by_video[i]) if i in num_roles_by_video.keys() else np.nan  \n",
    "      for i in range(video_info_df.shape[0]) ]\n",
    "video_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>num_roles_in_video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_1.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>395</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_2.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Blonde_being_brushed_brush_hair_f_nm_np2_ri_me...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brunette_Foxyanya_ultra_silky_long_hair_brushi...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Brushing_my_Long_Hair__February_2009_brush_hai...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>295</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename       class  width  \\\n",
       "1           April_09_brush_hair_u_nm_np1_ba_goo_1.avi  brush_hair    320   \n",
       "2           April_09_brush_hair_u_nm_np1_ba_goo_2.avi  brush_hair    320   \n",
       "12  Blonde_being_brushed_brush_hair_f_nm_np2_ri_me...  brush_hair    320   \n",
       "17  Brunette_Foxyanya_ultra_silky_long_hair_brushi...  brush_hair    416   \n",
       "32  Brushing_my_Long_Hair__February_2009_brush_hai...  brush_hair    320   \n",
       "\n",
       "    height  frames_per_second  num_frames  num_roles_in_video  \n",
       "1      240               30.0         395                 1.0  \n",
       "2      240               30.0         323                 1.0  \n",
       "12     240               30.0          79                 1.0  \n",
       "17     240               30.0         181                 1.0  \n",
       "32     240               30.0         295                 1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boo1 = video_info_df['num_roles_in_video']==1\n",
    "boo2 = video_info_df['num_frames']<=500\n",
    "video_info_df_ = video_info_df[(boo1)&(boo2)]\n",
    "video_info_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([391., 854., 194.,  68.,  34.,  26.,  11.,   8.,   6.,   3.]),\n",
       " array([ 22. ,  66.5, 111. , 155.5, 200. , 244.5, 289. , 333.5, 378. ,\n",
       "        422.5, 467. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASIUlEQVR4nO3db4xV+X3f8fcnsMb/EhmyA6KAC5FGSVirXjsj6tZVlAanEG9keLLSRHI0ipDIA9LYTaQIEqlWHiBtqipKHnQrIdvtSHGMpo4tkC2lQSRWVClaPOtdJwuYMDYbmEBh4sh10kgkkG8fzNn6AjPMgZm7Y37zfkmjc873/s693/t78LlnztwzJ1WFJKkt37faDUiSVp7hLkkNMtwlqUGGuyQ1yHCXpAatX+0GAJ5++unauXPnarchSU+Ul19++a+ramShx74nwn3nzp1MT0+vdhuS9ERJ8peLPeZpGUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatD3xBWqT6qdR7+0Kq/7+gvPrcrrSnpyeOQuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN6hXuSf5DkvNJXkvy2SRvTbIpyZkkl7vlxoHxx5LMJLmUZN/w2pckLWTJcE+yDfglYKyq3gOsA8aBo8DZqhoFznbbJNndPf4MsB94Mcm64bQvSVpI39My64G3JVkPvB24DhwAJrvHJ4GD3foB4GRV3a6qK8AMsGflWpYkLWXJcK+qvwL+M3AVuAH8n6r6Q2BLVd3oxtwANne7bAOuDTzFbFe7R5LDSaaTTM/NzS3vXUiS7tHntMxG5o/GdwH/DHhHko8+bJcFavVAoepEVY1V1djIyEjffiVJPfQ5LfMh4EpVzVXVPwKfB/41cDPJVoBueasbPwvsGNh/O/OncSRJb5I+4X4V+ECStycJsBe4CJwGJroxE8Cpbv00MJ5kQ5JdwChwbmXbliQ9zJI366iql5J8DvgqcAd4BTgBvBOYSnKI+Q+A57vx55NMARe68Ueq6u6Q+pckLaDXnZiq6hPAJ+4r32b+KH6h8ceB48trTZL0uLxCVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAb1uYfqDyd5deDnO0k+nmRTkjNJLnfLjQP7HEsyk+RSkn3DfQuSpPstGe5Vdamqnq2qZ4EfA/4e+AJwFDhbVaPA2W6bJLuBceAZYD/wYpJ1Q+pfkrSARz0tsxf4RlX9JXAAmOzqk8DBbv0AcLKqblfVFWAG2LMSzUqS+nnUcB8HPtutb6mqGwDdcnNX3wZcG9hntqvdI8nhJNNJpufm5h6xDUnSw/QO9yRvAT4C/I+lhi5QqwcKVSeqaqyqxkZGRvq2IUnq4VGO3H8a+GpV3ey2bybZCtAtb3X1WWDHwH7bgevLbVSS1N+jhPvP8t1TMgCngYlufQI4NVAfT7IhyS5gFDi33EYlSf2t7zMoyduBnwJ+YaD8AjCV5BBwFXgeoKrOJ5kCLgB3gCNVdXdFu5YkPVSvcK+qvwd+8L7at5j/9sxC448Dx5fdnSTpsXiFqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qFe5J3pXkc0m+nuRikn+VZFOSM0kud8uNA+OPJZlJcinJvuG1L0laSN8j998B/qCqfgR4L3AROAqcrapR4Gy3TZLdwDjwDLAfeDHJupVuXJK0uCXDPckPAD8OfAqgqv6hqr4NHAAmu2GTwMFu/QBwsqpuV9UVYAbYs9KNS5IW1+fI/YeAOeC/JXklySeTvAPYUlU3ALrl5m78NuDawP6zXe0eSQ4nmU4yPTc3t6w3IUm6V59wXw+8H/ivVfU+4P/SnYJZRBao1QOFqhNVNVZVYyMjI72alST10yfcZ4HZqnqp2/4c82F/M8lWgG55a2D8joH9twPXV6ZdSVIfS4Z7Vf1v4FqSH+5Ke4ELwGlgoqtNAKe69dPAeJINSXYBo8C5Fe1akvRQ63uO+/fAZ5K8Bfgm8PPMfzBMJTkEXAWeB6iq80mmmP8AuAMcqaq7K965JGlRvcK9ql4FxhZ4aO8i448Dx5fRlyRpGbxCVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAb1Cvckryf58ySvJpnuapuSnElyuVtuHBh/LMlMkktJ9g2reUnSwh7lyP3fVtWzVfXGTTuOAmerahQ4222TZDcwDjwD7AdeTLJuBXuWJC1hOadlDgCT3fokcHCgfrKqblfVFWAG2LOM15EkPaK+4V7AHyZ5Ocnhrralqm4AdMvNXX0bcG1g39muJkl6k/S9QfYHq+p6ks3AmSRff8jYLFCrBwbNf0gcBnj3u9/dsw1JUh+9jtyr6nq3vAV8gfnTLDeTbAXolre64bPAjoHdtwPXF3jOE1U1VlVjIyMjj/8OJEkPWDLck7wjyfe/sQ78O+A14DQw0Q2bAE5166eB8SQbkuwCRoFzK924JGlxfU7LbAG+kOSN8b9XVX+Q5CvAVJJDwFXgeYCqOp9kCrgA3AGOVNXdoXQvSVrQkuFeVd8E3rtA/VvA3kX2OQ4cX3Z3kqTH4hWqktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9Q73JOuSvJLki932piRnklzulhsHxh5LMpPkUpJ9w2hckrS4Rzly/xhwcWD7KHC2qkaBs902SXYD48AzwH7gxSTrVqZdSVIfvcI9yXbgOeCTA+UDwGS3PgkcHKifrKrbVXUFmAH2rEy7kqQ++h65/zbwq8A/DdS2VNUNgG65uatvA64NjJvtavdIcjjJdJLpubm5R25ckrS4JcM9yc8At6rq5Z7PmQVq9UCh6kRVjVXV2MjISM+nliT1sb7HmA8CH0nyYeCtwA8k+V3gZpKtVXUjyVbgVjd+FtgxsP924PpKNn2/nUe/NMynl6QnzpJH7lV1rKq2V9VO5v9Q+kdV9VHgNDDRDZsATnXrp4HxJBuS7AJGgXMr3rkkaVF9jtwX8wIwleQQcBV4HqCqzieZAi4Ad4AjVXV32Z1Kknp7pHCvqi8DX+7WvwXsXWTcceD4MnuTJD0mr1CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQX3uofrWJOeSfC3J+SS/0dU3JTmT5HK33Diwz7EkM0kuJdk3zDcgSXpQnyP328BPVtV7gWeB/Uk+ABwFzlbVKHC22ybJbuZvx/cMsB94Mcm6YTQvSVpYn3uoVlX9Xbf5VPdTwAFgsqtPAge79QPAyaq6XVVXgBlgz4p2LUl6qF7n3JOsS/IqcAs4U1UvAVuq6gZAt9zcDd8GXBvYfbar3f+ch5NMJ5mem5tbznuQJN2nV7hX1d2qehbYDuxJ8p6HDM9CT7HAc56oqrGqGhsZGenXrSSpl0f6tkxVfZv5G2TvB24m2QrQLW91w2aBHQO7bQeuL7tTSVJvfb4tM5LkXd3624APAV8HTgMT3bAJ4FS3fhoYT7IhyS5gFDi30o1Lkha3vseYrcBk942X7wOmquqLSf4UmEpyCLgKPA9QVeeTTAEXgDvAkaq6O5z2JUkLWTLcq+rPgPctUP8WsHeRfY4Dx5fdnSTpsXiFqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qcyemHUn+OMnFJOeTfKyrb0pyJsnlbrlxYJ9jSWaSXEqyb5hvQJL0oD5H7neAX6mqHwU+ABxJshs4CpytqlHgbLdN99g48Azz91p9sbuLkyTpTbJkuFfVjar6arf+t8BFYBtwAJjshk0CB7v1A8DJqrpdVVeAGWDPSjcuSVrcI51zT7KT+VvuvQRsqaobMP8BAGzuhm0Drg3sNtvV7n+uw0mmk0zPzc09eueSpEX1Dvck7wR+H/h4VX3nYUMXqNUDhaoTVTVWVWMjIyN925Ak9bDkDbIBkjzFfLB/pqo+35VvJtlaVTeSbAVudfVZYMfA7tuB6yvVsGDn0S+t2mu//sJzq/bakvrr822ZAJ8CLlbVbw08dBqY6NYngFMD9fEkG5LsAkaBcyvXsiRpKX2O3D8I/Bzw50le7Wq/BrwATCU5BFwFngeoqvNJpoALzH/T5khV3V3xziVJi1oy3Kvqf7HweXSAvYvscxw4voy+JEnL4BWqktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNajPnZg+neRWktcGapuSnElyuVtuHHjsWJKZJJeS7BtW45KkxfU5cv/vwP77akeBs1U1CpzttkmyGxgHnun2eTHJuhXrVpLUy5LhXlV/AvzNfeUDwGS3PgkcHKifrKrbVXUFmAH2rFCvkqSeHvec+5aqugHQLTd39W3AtYFxs11NkvQmWuk/qC50r9VacGByOMl0kum5ubkVbkOS1rbHDfebSbYCdMtbXX0W2DEwbjtwfaEnqKoTVTVWVWMjIyOP2YYkaSGPG+6ngYlufQI4NVAfT7IhyS5gFDi3vBYlSY9q/VIDknwW+Ang6SSzwCeAF4CpJIeAq8DzAFV1PskUcAG4AxypqrtD6l2StIglw72qfnaRh/YuMv44cHw5TUmSlscrVCWpQYa7JDVoydMy0qCdR7+0Kq/7+gvPrcrrSk8qj9wlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDfIiJj0RVuviKfACKj2ZDHdpCV6VqyeRp2UkqUGGuyQ1yHCXpAYN7Zx7kv3A7wDrgE9W1QvDei2pRf4RWcsxlHBPsg74L8BPMX/T7K8kOV1VF4bxepJW1mp+sKyGFj/MhnXkvgeYqapvAiQ5CRxg/t6qkvQ9pcXfkoYV7tuAawPbs8C/HByQ5DBwuNu8neS1IfXypHoa+OvVbuJ7iPPxIOfkXk/kfOQ3l7X7P1/sgWGFexao1T0bVSeAEwBJpqtqbEi9PJGck3s5Hw9yTu7lfNxrWN+WmQV2DGxvB64P6bUkSfcZVrh/BRhNsivJW4Bx4PSQXkuSdJ+hnJapqjtJfhH4n8x/FfLTVXX+IbucGEYfTzjn5F7Ox4Ock3s5HwNSVUuPkiQ9UbxCVZIaZLhLUoNWPdyT7E9yKclMkqOr3c+bIcmnk9wa/G5/kk1JziS53C03Djx2rJufS0n2rU7Xw5NkR5I/TnIxyfkkH+vqa3lO3prkXJKvdXPyG119zc4JzF/9nuSVJF/sttf0fDxUVa3aD/N/bP0G8EPAW4CvAbtXs6c36X3/OPB+4LWB2n8CjnbrR4Hf7NZ3d/OyAdjVzde61X4PKzwfW4H3d+vfD/xF977X8pwEeGe3/hTwEvCBtTwn3fv8ZeD3gC9222t6Ph72s9pH7v//3xRU1T8Ab/ybgqZV1Z8Af3Nf+QAw2a1PAgcH6ier6nZVXQFmmJ+3ZlTVjar6arf+t8BF5q9yXstzUlX1d93mU91PsYbnJMl24DngkwPlNTsfS1ntcF/o3xRsW6VeVtuWqroB82EHbO7qa2qOkuwE3sf8keqanpPuFMSrwC3gTFWt9Tn5beBXgX8aqK3l+Xio1Q73Jf9NgdbOHCV5J/D7wMer6jsPG7pArbk5qaq7VfUs81d470nynocMb3pOkvwMcKuqXu67ywK1Zuajj9UOd/9NwXfdTLIVoFve6uprYo6SPMV8sH+mqj7fldf0nLyhqr4NfBnYz9qdkw8CH0nyOvOnb38yye+ydudjSasd7v6bgu86DUx06xPAqYH6eJINSXYBo8C5VehvaJIE+BRwsap+a+ChtTwnI0ne1a2/DfgQ8HXW6JxU1bGq2l5VO5nPiT+qqo+yRuejl9X+iy7wYea/HfEN4NdXu5836T1/FrgB/CPzRxiHgB8EzgKXu+WmgfG/3s3PJeCnV7v/IczHv2H+V+Y/A17tfj68xufkXwCvdHPyGvAfu/qanZOB9/kTfPfbMmt+Phb78d8PSFKDVvu0jCRpCAx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KD/Bxqpq4zpOSBuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(video_info_df_['num_frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brush_hair': 19,\n",
       " 'cartwheel': 9,\n",
       " 'catch': 31,\n",
       " 'chew': 73,\n",
       " 'clap': 47,\n",
       " 'climb': 14,\n",
       " 'climb_stairs': 25,\n",
       " 'dive': 6,\n",
       " 'draw_sword': 57,\n",
       " 'dribble': 42,\n",
       " 'drink': 29,\n",
       " 'eat': 25,\n",
       " 'fall_floor': 8,\n",
       " 'flic_flac': 6,\n",
       " 'golf': 46,\n",
       " 'handstand': 18,\n",
       " 'hit': 48,\n",
       " 'hug': 1,\n",
       " 'jump': 30,\n",
       " 'kick': 4,\n",
       " 'kick_ball': 7,\n",
       " 'kiss': 5,\n",
       " 'laugh': 42,\n",
       " 'pick': 19,\n",
       " 'pour': 60,\n",
       " 'pullup': 51,\n",
       " 'punch': 14,\n",
       " 'push': 36,\n",
       " 'pushup': 18,\n",
       " 'ride_bike': 41,\n",
       " 'ride_horse': 5,\n",
       " 'run': 39,\n",
       " 'shoot_ball': 28,\n",
       " 'shoot_bow': 36,\n",
       " 'shoot_gun': 41,\n",
       " 'sit': 16,\n",
       " 'situp': 26,\n",
       " 'smile': 63,\n",
       " 'smoke': 50,\n",
       " 'somersault': 10,\n",
       " 'stand': 25,\n",
       " 'swing_baseball': 41,\n",
       " 'sword': 1,\n",
       " 'sword_exercise': 48,\n",
       " 'talk': 51,\n",
       " 'throw': 31,\n",
       " 'turn': 93,\n",
       " 'walk': 138,\n",
       " 'wave': 22}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = video_info_df_.groupby('class').count()['filename'].to_dict()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_selected = []\n",
    "for c in class_count:\n",
    "    if class_count[c]>20:\n",
    "        class_selected.append(c)\n",
    "len(class_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catch',\n",
       " 'chew',\n",
       " 'clap',\n",
       " 'climb_stairs',\n",
       " 'draw_sword',\n",
       " 'dribble',\n",
       " 'drink',\n",
       " 'eat',\n",
       " 'golf',\n",
       " 'hit',\n",
       " 'jump',\n",
       " 'laugh',\n",
       " 'pour',\n",
       " 'pullup',\n",
       " 'push',\n",
       " 'ride_bike',\n",
       " 'run',\n",
       " 'shoot_ball',\n",
       " 'shoot_bow',\n",
       " 'shoot_gun',\n",
       " 'situp',\n",
       " 'smile',\n",
       " 'smoke',\n",
       " 'stand',\n",
       " 'swing_baseball',\n",
       " 'sword_exercise',\n",
       " 'talk',\n",
       " 'throw',\n",
       " 'turn',\n",
       " 'walk',\n",
       " 'wave']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boo3 = video_info_df['class'].apply(lambda c: int(c in class_selected))\n",
    "video_info_df['class_selected(count>20)'] = boo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info_df['selected'] = (boo1)&(boo2)&(boo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info_df.to_csv(\"demo/video_info-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>num_roles_in_video</th>\n",
       "      <th>class_selected(count&gt;20)</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Ball_hochwerfen_-_Rolle_-_Ball_fangen_(Timo_3)...</td>\n",
       "      <td>catch</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Ballfangen_catch_u_cm_np1_fr_goo_1.avi</td>\n",
       "      <td>catch</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Ballfangen_catch_u_cm_np1_fr_goo_2.avi</td>\n",
       "      <td>catch</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Frisbee_catch_f_cm_np1_ri_med_1.avi</td>\n",
       "      <td>catch</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Goal_Keeping_Tips_catch_u_cm_np1_fr_med_1.avi</td>\n",
       "      <td>catch</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  class  width  height  \\\n",
       "216  Ball_hochwerfen_-_Rolle_-_Ball_fangen_(Timo_3)...  catch    320     240   \n",
       "218             Ballfangen_catch_u_cm_np1_fr_goo_1.avi  catch    320     240   \n",
       "219             Ballfangen_catch_u_cm_np1_fr_goo_2.avi  catch    320     240   \n",
       "234                Frisbee_catch_f_cm_np1_ri_med_1.avi  catch    320     240   \n",
       "237      Goal_Keeping_Tips_catch_u_cm_np1_fr_med_1.avi  catch    320     240   \n",
       "\n",
       "     frames_per_second  num_frames  num_roles_in_video  \\\n",
       "216               30.0          41                 1.0   \n",
       "218               30.0          37                 1.0   \n",
       "219               30.0          37                 1.0   \n",
       "234               30.0          54                 1.0   \n",
       "237               30.0          59                 1.0   \n",
       "\n",
       "     class_selected(count>20)  selected  \n",
       "216                         1      True  \n",
       "218                         1      True  \n",
       "219                         1      True  \n",
       "234                         1      True  \n",
       "237                         1      True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df_ = video_info_df[video_info_df['selected']]\n",
    "video_info_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1415, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 30, 30, 30])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = video_info_df_['class'].apply(lambda c: class_selected.index(c)).values\n",
    "target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df_['num_frames'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get KPs of Selected Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1415, 20, 1, 17, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num_frames = 20\n",
    "kps_ = []\n",
    "for i in video_info_df_.index:\n",
    "    row = video_info_df.iloc[i]\n",
    "    id_, class_ = str(i).zfill(4), row['class']\n",
    "    info_d = joblib.load(f\"{LOAD_KPS_PTH}/[{id_}]{class_}.joblib\")\n",
    "    kps = info_d['kps_buffer_selected']\n",
    "    #print(id_, class_, kps.shape)\n",
    "    num_frames = kps.shape[0]\n",
    "    np.random.seed(123)\n",
    "    idx = np.sort(np.random.permutation(range(num_frames))[:sample_num_frames]) \n",
    "    #print(idx)\n",
    "    kps = kps[idx]\n",
    "    #print(id_, class_, kps.shape)\n",
    "    kps[:,:,:,0] /= row['width']\n",
    "    kps[:,:,:,1] /= row['height']\n",
    "    kps_.append(kps[:,:,:,:2])\n",
    "kps_ = np.stack(kps_,0)\n",
    "kps_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995346, -0.0055555557)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_.max(), kps_.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1415, 20, 34)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_ = kps_.reshape(*kps_.shape[:2], -1)\n",
    "kps_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1415, 31)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_categorical(y, num_classes=31):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "target_catag = to_categorical(target)\n",
    "target_catag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((990, 31), (425, 31), (990,), (425,), (990, 20, 34), (425, 20, 34))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_catag_train, target_catag_test, target_train, target_test, input_train, input_test = \\\n",
    "    train_test_split(target_catag, target, kps_, test_size=0.3, random_state=42)\n",
    "target_catag_train.shape, target_catag_test.shape, target_train.shape, target_test.shape, \\\n",
    "input_train.shape, input_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes = 31,\n",
    "                 input_size = 34, \n",
    "                 hidden_size = 128, \n",
    "                 random_seed = 12345678):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn = nn.LSTMCell(input_size, hidden_size)\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.h0 = torch.rand(hidden_size,) \n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.c0 = torch.rand(hidden_size,) \n",
    "        \n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self , input_): # input_: (seq_len, batch_size, input_size) \n",
    "        seq_len, batch_size, input_size = input_.shape\n",
    "        assert input_size==self.input_size\n",
    "        #output = []\n",
    "        hx = torch.stack([self.h0]*batch_size,0) #(batch_size, hidden_size)\n",
    "        cx = torch.stack([self.c0]*batch_size,0) #(batch_size, hidden_size)\n",
    "        for inp in input_: # for each sent. in seq.   \n",
    "            hx, cx = self.rnn(inp, (hx, cx))    \n",
    "            #output.append(hx)\n",
    "        #output = torch.stack(output, 0)# output: (seq_len, batch_size, input_size)\n",
    "        #output = output.permute(1,2,0)# output: (batch_size, input_size, seq_len)\n",
    "        \n",
    "        output = self.cls(cx)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes = 31,\n",
    "                 input_size = 34, \n",
    "                 hidden_size = 128, \n",
    "                 random_seed = 12345678):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, dropout=0.3)\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.h0 = torch.rand(hidden_size,) \n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.c0 = torch.rand(hidden_size,) \n",
    "        \n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self , input_): # input_: (seq_len, batch_size, input_size) \n",
    "        seq_len, batch_size, input_size = input_.shape\n",
    "        assert input_size==self.input_size\n",
    "        #output = []\n",
    "        h0 = torch.stack([self.h0]*batch_size,0).unsqueeze(0) #(1, batch_size, hidden_size)\n",
    "        c0 = torch.stack([self.c0]*batch_size,0).unsqueeze(0) #(1, batch_size, hidden_size)\n",
    "        #print(h0.shape, c0.shape)\n",
    "        output, (hn, cn) = self.rnn(input_, (h0, c0))# output: (seq_len, batch_size, input_size)\n",
    "        #print(hn.shape, cn.shape)\n",
    "        output = self.cls(cn.squeeze(0))\n",
    "        #print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 20\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_catag_train, target_catag_test, target_train, target_test, input_train, input_test= \\\n",
    "map(torch.FloatTensor, [target_catag_train, target_catag_test, target_train, target_test, input_train, input_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = ActionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss:0.305541, acc_train:0.026263, acc_test:0.021176\n",
      "[39] loss:0.581310, acc_train:0.074747, acc_test:0.047059\n",
      "[59] loss:0.805295, acc_train:0.030303, acc_test:0.018824\n",
      "[79] loss:0.398962, acc_train:0.046465, acc_test:0.009412\n",
      "[99] loss:0.271569, acc_train:0.035354, acc_test:0.044706\n",
      "[119] loss:0.853942, acc_train:0.065657, acc_test:0.042353\n",
      "[139] loss:1.189216, acc_train:0.059596, acc_test:0.030588\n",
      "[159] loss:1.081008, acc_train:0.040404, acc_test:0.028235\n",
      "[179] loss:0.629394, acc_train:0.122222, acc_test:0.051765\n",
      "[199] loss:0.416741, acc_train:0.089899, acc_test:0.025882\n",
      "[219] loss:0.131153, acc_train:0.110101, acc_test:0.120000\n",
      "[239] loss:0.125573, acc_train:0.156566, acc_test:0.129412\n",
      "[259] loss:0.120540, acc_train:0.189899, acc_test:0.122353\n",
      "[279] loss:0.116979, acc_train:0.209091, acc_test:0.131765\n",
      "[299] loss:0.117244, acc_train:0.205051, acc_test:0.143529\n",
      "[319] loss:0.113710, acc_train:0.223232, acc_test:0.150588\n",
      "[339] loss:0.111346, acc_train:0.261616, acc_test:0.160000\n",
      "[359] loss:0.110728, acc_train:0.255556, acc_test:0.157647\n",
      "[379] loss:0.108740, acc_train:0.262626, acc_test:0.152941\n",
      "[399] loss:0.109295, acc_train:0.251515, acc_test:0.185882\n",
      "[419] loss:0.105000, acc_train:0.313131, acc_test:0.195294\n",
      "[439] loss:0.104447, acc_train:0.337374, acc_test:0.237647\n",
      "[459] loss:0.103544, acc_train:0.316162, acc_test:0.223529\n",
      "[479] loss:0.102280, acc_train:0.312121, acc_test:0.223529\n",
      "[499] loss:0.100885, acc_train:0.332323, acc_test:0.195294\n",
      "[519] loss:0.090242, acc_train:0.432323, acc_test:0.272941\n",
      "[539] loss:0.088514, acc_train:0.445455, acc_test:0.275294\n",
      "[559] loss:0.087199, acc_train:0.450505, acc_test:0.282353\n",
      "[579] loss:0.086016, acc_train:0.462626, acc_test:0.280000\n",
      "[599] loss:0.084909, acc_train:0.468687, acc_test:0.277647\n",
      "[619] loss:0.083850, acc_train:0.481818, acc_test:0.277647\n",
      "[639] loss:0.082821, acc_train:0.491919, acc_test:0.287059\n",
      "[659] loss:0.081825, acc_train:0.493939, acc_test:0.289412\n",
      "[679] loss:0.080817, acc_train:0.503030, acc_test:0.287059\n",
      "[699] loss:0.079945, acc_train:0.511111, acc_test:0.287059\n",
      "[719] loss:0.079130, acc_train:0.518182, acc_test:0.284706\n",
      "[739] loss:0.078347, acc_train:0.525253, acc_test:0.284706\n",
      "[759] loss:0.077598, acc_train:0.531313, acc_test:0.287059\n",
      "[779] loss:0.076879, acc_train:0.536364, acc_test:0.289412\n",
      "[799] loss:0.076189, acc_train:0.543434, acc_test:0.305882\n",
      "[819] loss:0.075527, acc_train:0.551515, acc_test:0.310588\n",
      "[839] loss:0.074888, acc_train:0.558586, acc_test:0.308235\n",
      "[859] loss:0.074273, acc_train:0.560606, acc_test:0.320000\n",
      "[879] loss:0.073669, acc_train:0.564646, acc_test:0.317647\n",
      "[899] loss:0.073082, acc_train:0.567677, acc_test:0.317647\n",
      "[919] loss:0.072507, acc_train:0.567677, acc_test:0.317647\n",
      "[939] loss:0.071946, acc_train:0.570707, acc_test:0.320000\n",
      "[959] loss:0.071396, acc_train:0.577778, acc_test:0.324706\n",
      "[979] loss:0.070864, acc_train:0.581818, acc_test:0.322353\n",
      "[999] loss:0.070336, acc_train:0.583838, acc_test:0.320000\n"
     ]
    }
   ],
   "source": [
    "epochs= 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epo in range(epochs):\n",
    "    if epo<200:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-2)\n",
    "    elif epo<500:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    output_train = model(input_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "    #print(logits.shape)\n",
    "    loss = loss_fn(output_train, target_catag_train.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epo%20==19:\n",
    "        model.eval()\n",
    "        accuracy_train = (torch.argmax(output_train.to('cpu'),1)==target_train).sum().numpy()/(target_train.shape[0])\n",
    "        output_test = model(input_test.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_test = (torch.argmax(output_test.to('cpu'),1)==target_test).sum().numpy()/(target_test.shape[0])\n",
    "        print(\"[{}] loss:{:.6f}, acc_train:{:4f}, acc_test:{:4f}\".format(epo, loss.item(), accuracy_train, accuracy_test))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss:0.094558, acc_train:0.390909, acc_test:0.197647\n",
      "[39] loss:0.087593, acc_train:0.461616, acc_test:0.272941\n",
      "[59] loss:0.085526, acc_train:0.436364, acc_test:0.277647\n",
      "[79] loss:0.088987, acc_train:0.410101, acc_test:0.237647\n",
      "[99] loss:0.084322, acc_train:0.436364, acc_test:0.258824\n",
      "[119] loss:0.084774, acc_train:0.501010, acc_test:0.277647\n",
      "[139] loss:0.083374, acc_train:0.467677, acc_test:0.265882\n",
      "[159] loss:0.082414, acc_train:0.469697, acc_test:0.308235\n",
      "[179] loss:0.085727, acc_train:0.460606, acc_test:0.251765\n",
      "[199] loss:0.083418, acc_train:0.454545, acc_test:0.247059\n",
      "[219] loss:0.066507, acc_train:0.603030, acc_test:0.343529\n",
      "[239] loss:0.065043, acc_train:0.613131, acc_test:0.341176\n",
      "[259] loss:0.063936, acc_train:0.618182, acc_test:0.352941\n",
      "[279] loss:0.063008, acc_train:0.627273, acc_test:0.357647\n",
      "[299] loss:0.062171, acc_train:0.635354, acc_test:0.355294\n",
      "[319] loss:0.061384, acc_train:0.642424, acc_test:0.350588\n",
      "[339] loss:0.060654, acc_train:0.647475, acc_test:0.352941\n",
      "[359] loss:0.059976, acc_train:0.660606, acc_test:0.355294\n",
      "[379] loss:0.059344, acc_train:0.664646, acc_test:0.352941\n",
      "[399] loss:0.058736, acc_train:0.668687, acc_test:0.352941\n",
      "[419] loss:0.058143, acc_train:0.676768, acc_test:0.352941\n",
      "[439] loss:0.057532, acc_train:0.678788, acc_test:0.352941\n",
      "[459] loss:0.056919, acc_train:0.683838, acc_test:0.350588\n",
      "[479] loss:0.056355, acc_train:0.688889, acc_test:0.352941\n",
      "[499] loss:0.055801, acc_train:0.689899, acc_test:0.357647\n",
      "[519] loss:0.085615, acc_train:0.480808, acc_test:0.240000\n",
      "[539] loss:0.080581, acc_train:0.475758, acc_test:0.305882\n",
      "[559] loss:0.075197, acc_train:0.535354, acc_test:0.261176\n",
      "[579] loss:0.077777, acc_train:0.534343, acc_test:0.275294\n",
      "[599] loss:0.078415, acc_train:0.497980, acc_test:0.263529\n",
      "[619] loss:0.078136, acc_train:0.495960, acc_test:0.301176\n",
      "[639] loss:0.080018, acc_train:0.526263, acc_test:0.308235\n",
      "[659] loss:0.078990, acc_train:0.507071, acc_test:0.296471\n",
      "[679] loss:0.073746, acc_train:0.518182, acc_test:0.308235\n",
      "[699] loss:0.075425, acc_train:0.538384, acc_test:0.277647\n",
      "[719] loss:0.054214, acc_train:0.711111, acc_test:0.362353\n",
      "[739] loss:0.052738, acc_train:0.717172, acc_test:0.362353\n",
      "[759] loss:0.051630, acc_train:0.723232, acc_test:0.371765\n",
      "[779] loss:0.050692, acc_train:0.737374, acc_test:0.371765\n",
      "[799] loss:0.049834, acc_train:0.745455, acc_test:0.371765\n",
      "[819] loss:0.049025, acc_train:0.752525, acc_test:0.371765\n",
      "[839] loss:0.048254, acc_train:0.754545, acc_test:0.371765\n",
      "[859] loss:0.047532, acc_train:0.756566, acc_test:0.371765\n",
      "[879] loss:0.046839, acc_train:0.760606, acc_test:0.369412\n",
      "[899] loss:0.046192, acc_train:0.764646, acc_test:0.371765\n",
      "[919] loss:0.045588, acc_train:0.767677, acc_test:0.376471\n",
      "[939] loss:0.045004, acc_train:0.772727, acc_test:0.376471\n",
      "[959] loss:0.044437, acc_train:0.776768, acc_test:0.374118\n",
      "[979] loss:0.043883, acc_train:0.782828, acc_test:0.374118\n",
      "[999] loss:0.043341, acc_train:0.783838, acc_test:0.374118\n"
     ]
    }
   ],
   "source": [
    "epochs= 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epo in range(epochs):\n",
    "    if epo<200:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    elif epo<500:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    elif epo<700:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    output_train = model(input_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "    #print(logits.shape)\n",
    "    loss = loss_fn(output_train, target_catag_train.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epo%20==19:\n",
    "        model.eval()\n",
    "        accuracy_train = (torch.argmax(output_train.to('cpu'),1)==target_train).sum().numpy()/(target_train.shape[0])\n",
    "        output_test = model(input_test.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_test = (torch.argmax(output_test.to('cpu'),1)==target_test).sum().numpy()/(target_test.shape[0])\n",
    "        print(\"[{}] loss:{:.6f}, acc_train:{:4f}, acc_test:{:4f}\".format(epo, loss.item(), accuracy_train, accuracy_test))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss:0.054142, acc_train:0.670707, acc_test:0.378824\n",
      "[39] loss:0.052368, acc_train:0.690909, acc_test:0.367059\n",
      "[59] loss:0.050968, acc_train:0.705051, acc_test:0.364706\n",
      "[79] loss:0.049846, acc_train:0.711111, acc_test:0.369412\n",
      "[99] loss:0.048828, acc_train:0.714141, acc_test:0.371765\n",
      "[119] loss:0.047893, acc_train:0.725253, acc_test:0.371765\n",
      "[139] loss:0.047040, acc_train:0.724242, acc_test:0.376471\n",
      "[159] loss:0.049152, acc_train:0.728283, acc_test:0.364706\n",
      "[179] loss:0.047593, acc_train:0.737374, acc_test:0.374118\n",
      "[199] loss:0.046444, acc_train:0.748485, acc_test:0.376471\n",
      "[219] loss:0.036758, acc_train:0.835354, acc_test:0.376471\n",
      "[239] loss:0.036157, acc_train:0.836364, acc_test:0.376471\n",
      "[259] loss:0.035655, acc_train:0.839394, acc_test:0.378824\n",
      "[279] loss:0.035195, acc_train:0.841414, acc_test:0.378824\n",
      "[299] loss:0.034766, acc_train:0.843434, acc_test:0.381176\n",
      "[319] loss:0.034356, acc_train:0.846465, acc_test:0.381176\n",
      "[339] loss:0.033962, acc_train:0.850505, acc_test:0.381176\n",
      "[359] loss:0.033580, acc_train:0.854545, acc_test:0.381176\n",
      "[379] loss:0.033208, acc_train:0.859596, acc_test:0.383529\n",
      "[399] loss:0.032843, acc_train:0.861616, acc_test:0.381176\n",
      "[419] loss:0.032486, acc_train:0.865657, acc_test:0.376471\n",
      "[439] loss:0.032135, acc_train:0.868687, acc_test:0.374118\n",
      "[459] loss:0.031790, acc_train:0.867677, acc_test:0.376471\n",
      "[479] loss:0.031453, acc_train:0.870707, acc_test:0.378824\n",
      "[499] loss:0.031119, acc_train:0.874747, acc_test:0.378824\n",
      "[519] loss:0.036997, acc_train:0.812121, acc_test:0.364706\n",
      "[539] loss:0.036120, acc_train:0.815152, acc_test:0.367059\n",
      "[559] loss:0.035405, acc_train:0.823232, acc_test:0.369412\n",
      "[579] loss:0.034780, acc_train:0.830303, acc_test:0.369412\n",
      "[599] loss:0.034218, acc_train:0.834343, acc_test:0.374118\n",
      "[619] loss:0.033702, acc_train:0.839394, acc_test:0.381176\n",
      "[639] loss:0.033217, acc_train:0.843434, acc_test:0.385882\n",
      "[659] loss:0.032736, acc_train:0.843434, acc_test:0.385882\n",
      "[679] loss:0.032228, acc_train:0.848485, acc_test:0.385882\n",
      "[699] loss:0.031772, acc_train:0.851515, acc_test:0.381176\n",
      "[719] loss:0.027366, acc_train:0.890909, acc_test:0.374118\n",
      "[739] loss:0.027062, acc_train:0.890909, acc_test:0.381176\n",
      "[759] loss:0.026789, acc_train:0.892929, acc_test:0.381176\n",
      "[779] loss:0.026532, acc_train:0.894949, acc_test:0.381176\n",
      "[799] loss:0.026285, acc_train:0.895960, acc_test:0.383529\n",
      "[819] loss:0.026045, acc_train:0.896970, acc_test:0.381176\n",
      "[839] loss:0.025812, acc_train:0.897980, acc_test:0.381176\n",
      "[859] loss:0.025584, acc_train:0.900000, acc_test:0.381176\n",
      "[879] loss:0.025361, acc_train:0.902020, acc_test:0.383529\n",
      "[899] loss:0.025142, acc_train:0.904040, acc_test:0.388235\n",
      "[919] loss:0.024927, acc_train:0.906061, acc_test:0.388235\n",
      "[939] loss:0.024716, acc_train:0.907071, acc_test:0.385882\n",
      "[959] loss:0.024508, acc_train:0.908081, acc_test:0.381176\n",
      "[979] loss:0.024302, acc_train:0.912121, acc_test:0.381176\n",
      "[999] loss:0.024100, acc_train:0.913131, acc_test:0.381176\n"
     ]
    }
   ],
   "source": [
    "epochs= 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epo in range(epochs):\n",
    "    if epo<200:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    elif epo<500:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    elif epo<700:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    output_train = model(input_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "    #print(logits.shape)\n",
    "    loss = loss_fn(output_train, target_catag_train.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epo%20==19:\n",
    "        model.eval()\n",
    "        accuracy_train = (torch.argmax(output_train.to('cpu'),1)==target_train).sum().numpy()/(target_train.shape[0])\n",
    "        output_test = model(input_test.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_test = (torch.argmax(output_test.to('cpu'),1)==target_test).sum().numpy()/(target_test.shape[0])\n",
    "        print(\"[{}] loss:{:.6f}, acc_train:{:4f}, acc_test:{:4f}\".format(epo, loss.item(), accuracy_train, accuracy_test))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"demo/model_selected.torchmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
