{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env with **Pytorch 1.6** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load KPs (See 4-2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_KPS_PTH = \"predicted_kps\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['predicted_kps/[0000]brush_hair.joblib',\n",
       "  'predicted_kps/[0001]brush_hair.joblib',\n",
       "  'predicted_kps/[0002]brush_hair.joblib',\n",
       "  'predicted_kps/[0003]brush_hair.joblib',\n",
       "  'predicted_kps/[0004]brush_hair.joblib'],\n",
       " ['predicted_kps/[6761]wave.joblib',\n",
       "  'predicted_kps/[6762]wave.joblib',\n",
       "  'predicted_kps/[6763]wave.joblib',\n",
       "  'predicted_kps/[6764]wave.joblib',\n",
       "  'predicted_kps/[6765]wave.joblib'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_filenames = sorted(glob.glob(f\"{LOAD_KPS_PTH}/*]*.joblib\"))\n",
    "kps_filenames[:5] , kps_filenames[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['width', 'height', 'frames_per_second', 'num_frames', 'num_role_register', 'boxes_buffer', 'kps_buffer', 'roles_buffer', 'frames_split_pos', 'cost_matrix_collection', 'image_size', 'bbox_area_by_role', 'num_frames_by_role', 'motion_distance_by_role', 'frames_by_role', 'completeness_by_role', 'selected_roles', 'boxes_buffer_selected', 'kps_buffer_selected'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_d = joblib.load(kps_filenames[1])\n",
    "info_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, (394, 17, 3), (394, 1, 17, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_d['num_frames'], info_d['kps_buffer'].shape, info_d['kps_buffer_selected'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_kps/[0000]brush_hair.joblib\n",
      "409 (415, 17, 3) (408, 1, 17, 3)\n",
      "  - num_role_register:  6\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0001]brush_hair.joblib\n",
      "395 (394, 17, 3) (394, 1, 17, 3)\n",
      "  - num_role_register:  1\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0002]brush_hair.joblib\n",
      "323 (322, 17, 3) (322, 1, 17, 3)\n",
      "  - num_role_register:  1\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n",
      "predicted_kps/[0003]brush_hair.joblib\n",
      "246 (281, 17, 3) (245, 1, 17, 3)\n",
      "  - num_role_register:  26\n",
      "  - selected_roles:  [7]\n",
      "****************************************************************\n",
      "predicted_kps/[0004]brush_hair.joblib\n",
      "159 (169, 17, 3) (158, 1, 17, 3)\n",
      "  - num_role_register:  6\n",
      "  - selected_roles:  [0]\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "for kpfn in kps_filenames[:5]:\n",
    "    info_d = joblib.load(kpfn)\n",
    "    print(kpfn)\n",
    "    #print(info_d.keys())\n",
    "    print(info_d['num_frames'], info_d['kps_buffer'].shape, info_d['kps_buffer_selected'].shape)\n",
    "    print('  - num_role_register: ',info_d['num_role_register'])\n",
    "    #print('  - num_frames_by_role: ', info_d['num_frames_by_role'])\n",
    "    #print('  - motion_distance_by_role: ', info_d['motion_distance_by_role'])\n",
    "    #print('  - completeness_by_role: ', info_d['completeness_by_role'])\n",
    "    print('  - selected_roles: ', info_d['selected_roles'])\n",
    "    print('*'*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get num_roles for each Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5796/5796 [00:28<00:00, 201.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "num_roles_by_video = {}\n",
    "\n",
    "for j, kpfn in enumerate(tqdm.tqdm(kps_filenames)):\n",
    "        id_, class_ = kpfn.split('/')[-1].split('.')[0][1:].split(']') # \"6765\", \"wave\"\n",
    "        #print(id_, class_)\n",
    "        info_d = joblib.load(kpfn)\n",
    "        n_roles = len(info_d['num_frames_by_role'])\n",
    "        id_ = int(id_)\n",
    "        num_roles_by_video.update({id_:n_roles})\n",
    "        \n",
    "#joblib.dump(video_selected, f\"demo/video_selected.joblib\")\n",
    "#joblib.dump(num_roles_by_video, f\"demo/num_roles_by_video.joblib\")\n",
    "#video_selected = joblib.load(f\"demo/video_selected.joblib\")\n",
    "#num_roles_by_video = joblib.load(f\"demo/num_roles_by_video.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有[5796]支成功辨識的video\n"
     ]
    }
   ],
   "source": [
    "print(f\"共有[{len(num_roles_by_video)}]支成功辨識的video\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_roles_by_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset Overview (See 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_0.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_1.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_2.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename       class  width  \\\n",
       "0          April_09_brush_hair_u_nm_np1_ba_goo_0.avi  brush_hair    320   \n",
       "1          April_09_brush_hair_u_nm_np1_ba_goo_1.avi  brush_hair    320   \n",
       "2          April_09_brush_hair_u_nm_np1_ba_goo_2.avi  brush_hair    320   \n",
       "3  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "4  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "\n",
       "   height  frames_per_second  num_frames  \n",
       "0     240               30.0         409  \n",
       "1     240               30.0         395  \n",
       "2     240               30.0         323  \n",
       "3     240               30.0         246  \n",
       "4     240               30.0         159  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df = pd.read_csv(\"demo/video_info.csv\", index_col=0)\n",
    "video_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frames_per_second</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>num_roles_in_video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_0.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>409</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_1.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>395</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April_09_brush_hair_u_nm_np1_ba_goo_2.avi</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>246</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...</td>\n",
       "      <td>brush_hair</td>\n",
       "      <td>416</td>\n",
       "      <td>240</td>\n",
       "      <td>30.0</td>\n",
       "      <td>159</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename       class  width  \\\n",
       "0          April_09_brush_hair_u_nm_np1_ba_goo_0.avi  brush_hair    320   \n",
       "1          April_09_brush_hair_u_nm_np1_ba_goo_1.avi  brush_hair    320   \n",
       "2          April_09_brush_hair_u_nm_np1_ba_goo_2.avi  brush_hair    320   \n",
       "3  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "4  Aussie_Brunette_Brushing_Hair_II_brush_hair_u_...  brush_hair    416   \n",
       "\n",
       "   height  frames_per_second  num_frames  num_roles_in_video  \n",
       "0     240               30.0         409                 6.0  \n",
       "1     240               30.0         395                 1.0  \n",
       "2     240               30.0         323                 1.0  \n",
       "3     240               30.0         246                26.0  \n",
       "4     240               30.0         159                 6.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "video_info_df['num_roles_in_video'] = \\\n",
    "    [ int(num_roles_by_video[i]) if i in num_roles_by_video.keys() else np.nan  \n",
    "      for i in range(video_info_df.shape[0]) ]\n",
    "video_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "boo1 = video_info_df['num_roles_in_video']==1\n",
    "boo2 = video_info_df['num_frames']<=500\n",
    "video_info_df_ = video_info_df[(boo1)&(boo2)]\n",
    "video_info_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.844e+03, 6.880e+02, 1.790e+02, 6.000e+01, 1.500e+01, 4.000e+00,\n",
       "        4.000e+00, 0.000e+00, 0.000e+00, 2.000e+00]),\n",
       " array([  19. ,  123.4,  227.8,  332.2,  436.6,  541. ,  645.4,  749.8,\n",
       "         854.2,  958.6, 1063. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(video_info_df['num_frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(video_info_df['num_frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brush_hair': 86,\n",
       " 'cartwheel': 89,\n",
       " 'catch': 97,\n",
       " 'chew': 106,\n",
       " 'clap': 125,\n",
       " 'climb': 70,\n",
       " 'climb_stairs': 91,\n",
       " 'dive': 84,\n",
       " 'draw_sword': 98,\n",
       " 'dribble': 139,\n",
       " 'drink': 142,\n",
       " 'eat': 95,\n",
       " 'fall_floor': 93,\n",
       " 'fencing': 112,\n",
       " 'flic_flac': 59,\n",
       " 'golf': 104,\n",
       " 'handstand': 99,\n",
       " 'hit': 106,\n",
       " 'hug': 116,\n",
       " 'jump': 117,\n",
       " 'kick': 116,\n",
       " 'kick_ball': 105,\n",
       " 'kiss': 80,\n",
       " 'laugh': 125,\n",
       " 'pick': 95,\n",
       " 'pour': 101,\n",
       " 'pullup': 95,\n",
       " 'punch': 96,\n",
       " 'push': 94,\n",
       " 'pushup': 81,\n",
       " 'ride_bike': 79,\n",
       " 'ride_horse': 74,\n",
       " 'run': 159,\n",
       " 'shake_hands': 161,\n",
       " 'shoot_ball': 126,\n",
       " 'shoot_bow': 103,\n",
       " 'shoot_gun': 90,\n",
       " 'sit': 127,\n",
       " 'situp': 88,\n",
       " 'smile': 94,\n",
       " 'smoke': 103,\n",
       " 'somersault': 111,\n",
       " 'stand': 130,\n",
       " 'swing_baseball': 134,\n",
       " 'sword': 110,\n",
       " 'sword_exercise': 116,\n",
       " 'talk': 106,\n",
       " 'throw': 91,\n",
       " 'turn': 223,\n",
       " 'walk': 460,\n",
       " 'wave': 95}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = video_info_df.groupby('class').count()['filename'].to_dict()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_selected = []\n",
    "for c in class_count:\n",
    "    if class_count[c]>20:\n",
    "        class_selected.append(c)\n",
    "len(class_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brush_hair',\n",
       " 'cartwheel',\n",
       " 'catch',\n",
       " 'chew',\n",
       " 'clap',\n",
       " 'climb',\n",
       " 'climb_stairs',\n",
       " 'dive',\n",
       " 'draw_sword',\n",
       " 'dribble',\n",
       " 'drink',\n",
       " 'eat',\n",
       " 'fall_floor',\n",
       " 'fencing',\n",
       " 'flic_flac',\n",
       " 'golf',\n",
       " 'handstand',\n",
       " 'hit',\n",
       " 'hug',\n",
       " 'jump',\n",
       " 'kick',\n",
       " 'kick_ball',\n",
       " 'kiss',\n",
       " 'laugh',\n",
       " 'pick',\n",
       " 'pour',\n",
       " 'pullup',\n",
       " 'punch',\n",
       " 'push',\n",
       " 'pushup',\n",
       " 'ride_bike',\n",
       " 'ride_horse',\n",
       " 'run',\n",
       " 'shake_hands',\n",
       " 'shoot_ball',\n",
       " 'shoot_bow',\n",
       " 'shoot_gun',\n",
       " 'sit',\n",
       " 'situp',\n",
       " 'smile',\n",
       " 'smoke',\n",
       " 'somersault',\n",
       " 'stand',\n",
       " 'swing_baseball',\n",
       " 'sword',\n",
       " 'sword_exercise',\n",
       " 'talk',\n",
       " 'throw',\n",
       " 'turn',\n",
       " 'walk',\n",
       " 'wave']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 50, 50, 50])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = video_info_df['class'].apply(lambda c: class_selected.index(c)).values\n",
    "target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info_df['num_frames'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get KPs of Selected Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5796, 80, 1, 17, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num_frames = 80 # seq_len \n",
    "kps_ = []\n",
    "for i in video_info_df.index:\n",
    "    row = video_info_df.loc[i]\n",
    "    id_, class_ = str(i).zfill(4), row['class']\n",
    "    info_d = joblib.load(f\"{LOAD_KPS_PTH}/[{id_}]{class_}.joblib\")\n",
    "    kps = info_d['kps_buffer_selected']\n",
    "    #print(id_, class_, kps.shape)\n",
    "    num_frames = kps.shape[0]\n",
    "    if num_frames>=sample_num_frames:\n",
    "        np.random.seed(123)\n",
    "        a1 = np.random.permutation(range(num_frames))\n",
    "        idx = np.sort(a1[:sample_num_frames]) \n",
    "    else:\n",
    "        np.random.seed(123)\n",
    "        a1 = np.random.permutation(range(num_frames))\n",
    "        a2 = np.random.choice(range(num_frames),sample_num_frames-num_frames)\n",
    "        idx = np.sort( np.concatenate([a1,a2]) )         \n",
    "    #print(idx)\n",
    "    kps = kps[idx]\n",
    "    #print(id_, class_, kps.shape)\n",
    "    kps[:,:,:,0] /= row['width']\n",
    "    kps[:,:,:,1] /= row['height']\n",
    "    kps_.append(kps[:,:,:,:2])\n",
    "kps_ = np.stack(kps_,0)\n",
    "kps_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999628, -0.0056818184)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_.max(), kps_.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5796, 80, 34)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_ = kps_.reshape(*kps_.shape[:2], -1)\n",
    "kps_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset  using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5796, 51)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "target_catag = to_categorical(target, len(class_selected))\n",
    "target_catag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4057, 51), (1739, 51), (4057,), (1739,), (4057, 80, 34), (1739, 80, 34))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_catag_train, target_catag_test, target_train, target_test, input_train, input_test = \\\n",
    "    train_test_split(target_catag, target, kps_, test_size=0.3, random_state=42)\n",
    "target_catag_train.shape, target_catag_test.shape, target_train.shape, target_test.shape, \\\n",
    "input_train.shape, input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "target_catag_train, target_catag_test, target_train, target_test, input_train, input_test= \\\n",
    "map(torch.FloatTensor, [target_catag_train, target_catag_test, target_train, target_test, input_train, input_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class KPsDataset(Dataset):\n",
    "    def __init__(self, input_train, target_catag_train, transform=None):\n",
    "        self.X = input_train\n",
    "        self.y = target_catag_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  KPsDataset(input_train, target_catag_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, \n",
    "                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 80, 34]) torch.Size([32, 51])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape); break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes = 31,\n",
    "                 input_size = 34, \n",
    "                 hidden_size = 128, \n",
    "                 random_seed = 12345678):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn = nn.LSTMCell(input_size, hidden_size)\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.h0 = torch.rand(hidden_size,) \n",
    "        torch.random.manual_seed(random_seed)\n",
    "        self.c0 = torch.rand(hidden_size,) \n",
    "        \n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self , input_): # input_: (seq_len, batch_size, input_size) \n",
    "        seq_len, batch_size, input_size = input_.shape\n",
    "        assert input_size==self.input_size\n",
    "        #output = []\n",
    "        hx = torch.stack([self.h0]*batch_size,0) #(batch_size, hidden_size)\n",
    "        cx = torch.stack([self.c0]*batch_size,0) #(batch_size, hidden_size)\n",
    "        for inp in input_: # for each sent. in seq.   \n",
    "            hx, cx = self.rnn(inp, (hx, cx))    \n",
    "            #output.append(hx)\n",
    "        #output = torch.stack(output, 0)# output: (seq_len, batch_size, input_size)\n",
    "        #output = output.permute(1,2,0)# output: (batch_size, input_size, seq_len)\n",
    "        \n",
    "        output = self.cls(cx)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes = len(class_selected),\n",
    "                 input_size = 34, \n",
    "                 hidden_size = 128, \n",
    "                 num_layers=2,\n",
    "                 random_seed = 12345678,\n",
    "                 device=\"cpu\"\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.random_seed = random_seed\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.7)        \n",
    "        self.cls = nn.Linear(hidden_size*2, num_classes)\n",
    "        \n",
    "    def forward(self , input_): # input_: (seq_len, batch_size, input_size) \n",
    "        seq_len, batch_size, input_size = input_.shape\n",
    "        assert input_size==self.input_size\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        h0 = torch.rand(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        c0 = torch.rand(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        #print(h0.shape, c0.shape)\n",
    "        output, (hn, cn) = self.rnn(input_, (h0, c0))# output: (seq_len, batch_size, hidden_size)\n",
    "        #print(hn.shape, cn.shape)\n",
    "        #print(output.shape)\n",
    "        output = torch.mean(output,0)# output: (batch_size, hidden_size)\n",
    "        output = torch.cat([cn.mean(0),output],1)\n",
    "        #print(output.shape)\n",
    "        output = self.cls(output) # output: (batch_size, num_classes)\n",
    "        #print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActionClassifier(device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss:0.074668, acc_train:0.173774, acc_test:0.165037\n",
      "[39] loss:0.065214, acc_train:0.274341, acc_test:0.235193\n",
      "[59] loss:0.061392, acc_train:0.336209, acc_test:0.262795\n",
      "[79] loss:0.068919, acc_train:0.415085, acc_test:0.285796\n",
      "[99] loss:0.072347, acc_train:0.498891, acc_test:0.286371\n",
      "[119] loss:0.073058, acc_train:0.579246, acc_test:0.303048\n",
      "[139] loss:0.079677, acc_train:0.593049, acc_test:0.304198\n",
      "[159] loss:0.078743, acc_train:0.601923, acc_test:0.297297\n",
      "[179] loss:0.079037, acc_train:0.627311, acc_test:0.296722\n",
      "[199] loss:0.077629, acc_train:0.639882, acc_test:0.303048\n",
      "[219] loss:0.073370, acc_train:0.545970, acc_test:0.278896\n",
      "[239] loss:0.066422, acc_train:0.635445, acc_test:0.285796\n",
      "[259] loss:0.078438, acc_train:0.641114, acc_test:0.270270\n",
      "[279] loss:0.080692, acc_train:0.674883, acc_test:0.277171\n",
      "[299] loss:0.097183, acc_train:0.709638, acc_test:0.273145\n",
      "[319] loss:0.085208, acc_train:0.829677, acc_test:0.304773\n",
      "[339] loss:0.080225, acc_train:0.834853, acc_test:0.318574\n",
      "[359] loss:0.074136, acc_train:0.838551, acc_test:0.310523\n",
      "[379] loss:0.077737, acc_train:0.839044, acc_test:0.312823\n",
      "[399] loss:0.071228, acc_train:0.840769, acc_test:0.307648\n",
      "[419] loss:0.091729, acc_train:0.761400, acc_test:0.282921\n",
      "[439] loss:0.071568, acc_train:0.787774, acc_test:0.305923\n",
      "[459] loss:0.074608, acc_train:0.808233, acc_test:0.306498\n",
      "[479] loss:0.091285, acc_train:0.796155, acc_test:0.296147\n",
      "[499] loss:0.073455, acc_train:0.813162, acc_test:0.301898\n",
      "[519] loss:0.070506, acc_train:0.880454, acc_test:0.316274\n",
      "[539] loss:0.057326, acc_train:0.889081, acc_test:0.325474\n",
      "[559] loss:0.048354, acc_train:0.883658, acc_test:0.319149\n",
      "[579] loss:0.043264, acc_train:0.882672, acc_test:0.316274\n",
      "[599] loss:0.042644, acc_train:0.885137, acc_test:0.318574\n",
      "[619] loss:0.067545, acc_train:0.821543, acc_test:0.310523\n",
      "[639] loss:0.061863, acc_train:0.810698, acc_test:0.305923\n",
      "[659] loss:0.071188, acc_train:0.804535, acc_test:0.299022\n",
      "[679] loss:0.043550, acc_train:0.827212, acc_test:0.296722\n",
      "[699] loss:0.039562, acc_train:0.856298, acc_test:0.304198\n",
      "[719] loss:0.028297, acc_train:0.896475, acc_test:0.316849\n",
      "[739] loss:0.024255, acc_train:0.897461, acc_test:0.316274\n",
      "[759] loss:0.020460, acc_train:0.894996, acc_test:0.324899\n",
      "[779] loss:0.018636, acc_train:0.892038, acc_test:0.323174\n",
      "[799] loss:0.013750, acc_train:0.894750, acc_test:0.319149\n",
      "[819] loss:0.048879, acc_train:0.844466, acc_test:0.309373\n",
      "[839] loss:0.034802, acc_train:0.848410, acc_test:0.308223\n",
      "[859] loss:0.041567, acc_train:0.865418, acc_test:0.308223\n",
      "[879] loss:0.031142, acc_train:0.865664, acc_test:0.302473\n",
      "[899] loss:0.017207, acc_train:0.864925, acc_test:0.300748\n",
      "[919] loss:0.016353, acc_train:0.905102, acc_test:0.321449\n",
      "[939] loss:0.008164, acc_train:0.906335, acc_test:0.319149\n",
      "[959] loss:0.005060, acc_train:0.906088, acc_test:0.327200\n",
      "[979] loss:0.004646, acc_train:0.905349, acc_test:0.323749\n",
      "[999] loss:0.006220, acc_train:0.901158, acc_test:0.319724\n"
     ]
    }
   ],
   "source": [
    "epochs= 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epo in range(epochs):\n",
    "    if (epo//100)%2==0 :\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    elif (epo//100)%2==1:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        \n",
    "    for X_train, y_train in train_loader:\n",
    "        output_train = model(X_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "        loss = loss_fn(output_train, y_train.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epo%20==19:\n",
    "        model.eval()\n",
    "        output_train = model(input_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_train = (torch.argmax(output_train.to('cpu'),1)==target_train).sum().numpy()/(target_train.shape[0])\n",
    "        output_test = model(input_test.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_test = (torch.argmax(output_test.to('cpu'),1)==target_test).sum().numpy()/(target_test.shape[0])\n",
    "        print(\"[{}] loss:{:.6f}, acc_train:{:4f}, acc_test:{:4f}\".format(epo, loss.item(), accuracy_train, accuracy_test))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss:0.071942, acc_train:0.390436, acc_test:0.276596\n",
      "[39] loss:0.078308, acc_train:0.463643, acc_test:0.268545\n",
      "[59] loss:0.080207, acc_train:0.510722, acc_test:0.274296\n",
      "[79] loss:0.079274, acc_train:0.567907, acc_test:0.270270\n",
      "[99] loss:0.081542, acc_train:0.553611, acc_test:0.255319\n",
      "[119] loss:0.072337, acc_train:0.559773, acc_test:0.255894\n",
      "[139] loss:0.067999, acc_train:0.563471, acc_test:0.253594\n",
      "[159] loss:0.078404, acc_train:0.539808, acc_test:0.239793\n",
      "[179] loss:0.062446, acc_train:0.528716, acc_test:0.226567\n",
      "[199] loss:0.067423, acc_train:0.579985, acc_test:0.268545\n",
      "[219] loss:0.045956, acc_train:0.675376, acc_test:0.276021\n",
      "[239] loss:0.043753, acc_train:0.681785, acc_test:0.277746\n",
      "[259] loss:0.033687, acc_train:0.678827, acc_test:0.277746\n",
      "[279] loss:0.022841, acc_train:0.687207, acc_test:0.280046\n",
      "[299] loss:0.022599, acc_train:0.682031, acc_test:0.275446\n",
      "[319] loss:0.013867, acc_train:0.686714, acc_test:0.273145\n",
      "[339] loss:0.012339, acc_train:0.690658, acc_test:0.280046\n",
      "[359] loss:0.012459, acc_train:0.694848, acc_test:0.282346\n",
      "[379] loss:0.008852, acc_train:0.690905, acc_test:0.273145\n",
      "[399] loss:0.009333, acc_train:0.690905, acc_test:0.278321\n",
      "[419] loss:0.005953, acc_train:0.691644, acc_test:0.278321\n",
      "[439] loss:0.003115, acc_train:0.694109, acc_test:0.282346\n",
      "[459] loss:0.003655, acc_train:0.690905, acc_test:0.278896\n",
      "[479] loss:0.003855, acc_train:0.696820, acc_test:0.288097\n",
      "[499] loss:0.003797, acc_train:0.698299, acc_test:0.287522\n",
      "[519] loss:0.002408, acc_train:0.702243, acc_test:0.282346\n",
      "[539] loss:0.001559, acc_train:0.701011, acc_test:0.281771\n",
      "[559] loss:0.002829, acc_train:0.706433, acc_test:0.285221\n",
      "[579] loss:0.001143, acc_train:0.707666, acc_test:0.281771\n",
      "[599] loss:0.001534, acc_train:0.703475, acc_test:0.282346\n",
      "[619] loss:0.001040, acc_train:0.707912, acc_test:0.282346\n",
      "[639] loss:0.001003, acc_train:0.704954, acc_test:0.281196\n",
      "[659] loss:0.000825, acc_train:0.702736, acc_test:0.284071\n",
      "[679] loss:0.001699, acc_train:0.706926, acc_test:0.282921\n",
      "[699] loss:0.001655, acc_train:0.707419, acc_test:0.280621\n",
      "[719] loss:0.001612, acc_train:0.704708, acc_test:0.282346\n",
      "[739] loss:0.001779, acc_train:0.704954, acc_test:0.288672\n",
      "[759] loss:0.000894, acc_train:0.704461, acc_test:0.282921\n",
      "[779] loss:0.000867, acc_train:0.703968, acc_test:0.282921\n",
      "[799] loss:0.001902, acc_train:0.705447, acc_test:0.285221\n",
      "[819] loss:0.000580, acc_train:0.704708, acc_test:0.280046\n",
      "[839] loss:0.000838, acc_train:0.706680, acc_test:0.283496\n",
      "[859] loss:0.001363, acc_train:0.703722, acc_test:0.286371\n",
      "[879] loss:0.002458, acc_train:0.707173, acc_test:0.282921\n",
      "[899] loss:0.001576, acc_train:0.704461, acc_test:0.282346\n",
      "[919] loss:0.000603, acc_train:0.704954, acc_test:0.281196\n",
      "[939] loss:0.000807, acc_train:0.707912, acc_test:0.284646\n",
      "[959] loss:0.001018, acc_train:0.706680, acc_test:0.282346\n",
      "[979] loss:0.001205, acc_train:0.701997, acc_test:0.281771\n",
      "[999] loss:0.000415, acc_train:0.704215, acc_test:0.281771\n"
     ]
    }
   ],
   "source": [
    "epochs= 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epo in range(epochs):\n",
    "    if epo<200 :\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
    "    elif epo<500:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)        \n",
    "        \n",
    "    for X_train, y_train in train_loader:\n",
    "        output_train = model(X_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "        loss = loss_fn(output_train, y_train.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epo%20==19:\n",
    "        model.eval()\n",
    "        output_train = model(input_train.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_train = (torch.argmax(output_train.to('cpu'),1)==target_train).sum().numpy()/(target_train.shape[0])\n",
    "        output_test = model(input_test.permute(1,0,2).to(device))#(1415, 31)\n",
    "        accuracy_test = (torch.argmax(output_test.to('cpu'),1)==target_test).sum().numpy()/(target_test.shape[0])\n",
    "        print(\"[{}] loss:{:.6f}, acc_train:{:4f}, acc_test:{:4f}\".format(epo, loss.item(), accuracy_train, accuracy_test))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"demo/model.torchmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
